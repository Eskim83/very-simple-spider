# Very Simple Spider 2.0 ğŸ•·ï¸

Prosty i rozszerzalny pajÄ…k do pobierania stron internetowych, napisany w PHP 8.2+. ObsÅ‚uguje robots.txt, filtrowanie MIME, limitowanie gÅ‚Ä™bokoÅ›ci, liczby stron i rozmiaru danych. Idealny do maÅ‚ych crawlerÃ³w lub integracji z systemami ETL.

**ğŸ”– Wersja:** 2.0  
ğŸ“š Wersja 1.0 + artykuÅ‚: [Pobieranie strony offline w PHP](https://eskim.pl/pobieranie-strony-offline-w-php/)  
ğŸŒ Strona autora: [https://eskim.pl](https://eskim.pl)  
â˜• Donate: [https://buymeacoffee.com/eskim](https://buymeacoffee.com/eskim)  
ğŸ“œ Licencja: [GNU GPL v2.0](https://www.gnu.org/licenses/gpl-2.0.html)

---

## ğŸ“¦ Instalacja

```bash
git clone https://github.com/eskim83/very-simple-spider.git
cd very-simple-spider
composer install
```

## ğŸš€ UÅ¼ycie (CLI)

```bash
php bin/crawl.php --url=https://example.com [--depth=2] [--silent] [--limit=100] [--max-bytes=500000] [--out=result.json]
```

### PrzykÅ‚ad:
```bash
php bin/crawl.php --url=https://eskim.pl --depth=2 --limit=10 --max-bytes=200000 --out=eskim.json
```

## âš™ï¸ Opcje

| Parametr        | Opis                                      |
|-----------------|-------------------------------------------|
| `--url`         | Adres URL startowy (wymagany)             |
| `--depth=N`     | Maksymalna gÅ‚Ä™bokoÅ›Ä‡ linkÃ³w (domyÅ›lnie 3) |
| `--silent`      | WyÅ‚Ä…cza logowanie                         |
| `--limit=N`     | Limit liczby stron                        |
| `--max-bytes=N` | Limit rozmiaru pobranych danych (B)       |
| `--out=plik.json` | Zapis wynikÃ³w crawl'a do pliku JSON     |

## ğŸ§ª Testy

### Instalacja PHPUnit
```bash
composer require --dev phpunit/phpunit
```

### Uruchamianie testÃ³w:
```bash
php vendor/bin/phpunit --bootstrap tests/bootstrap.php tests/
```

## ğŸ“ Struktura katalogÃ³w

```
bin/             # CLI (crawl.php)
src/             # Klasy Spidera
tests/           # Jednostkowe i integracyjne testy
composer.json    # Autoloading i zaleÅ¼noÅ›ci
README.md
```

## âœ… Funkcje

- Rekurencyjne pobieranie stron
- ObsÅ‚uga `robots.txt`
- Pomijanie plikÃ³w statycznych (js, css, svg, pdf itd.)
- Filtr URL-i z callbackiem
- Limitowanie stron, gÅ‚Ä™bokoÅ›ci, danych
- ObsÅ‚uga Guzzle lub `file_get_contents`
- Zapisywanie wynikÃ³w do JSON

## ğŸª› Wymagania

- PHP 8.2+
- Composer
- (Opcjonalnie) GuzzleHttp

## ğŸ›‘ Zrzeczenie odpowiedzialnoÅ›ci

Proces pozyskiwania danych z serwisÃ³w opiera siÄ™ na Web Scrapping-u i moÅ¼e byÄ‡ uznawany za szkodliwy przez niektÃ³re strony internetowe.  
**Autor nie ponosi Å¼adnej odpowiedzialnoÅ›ci za uÅ¼ycie tego oprogramowania, skutki jego dziaÅ‚ania ani naruszenie regulaminÃ³w zewnÄ™trznych serwisÃ³w.**

**Korzystasz na wÅ‚asnÄ… odpowiedzialnoÅ›Ä‡.**

## ğŸ“ Licencja

Projekt objÄ™ty licencjÄ… [GNU GPL v2.0](https://www.gnu.org/licenses/gpl-2.0.html)
